{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c074a60a078dedc",
   "metadata": {},
   "source": [
    "# Tabular Results\n",
    "\n",
    "This notebook allows us to query tabular results via [`mlflow`](https://mlflow.org/). As a prerequisite, the experiments have to be performed in a first step. If this is the case, we can load the results for the different tables in the accompanied article. Update the `mlruns_path` to the path used in your config file [`experiment.yaml`](../conf/experiment.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hydra.utils import to_absolute_path\n",
    "from mlflow import set_tracking_uri, get_experiment_by_name, search_runs\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# TODO: Adjust `MLRUNS_PATH`.\n",
    "MLRUNS_PATH = \".\"\n",
    "\n",
    "# Flag for showing results in LaTex format.\n",
    "SHOW_LATEX = False\n",
    "\n",
    "# Multi-annotator learning approaches and variants of dopanim.\n",
    "APPROACHES = [\"ground-truth\", \"majority-vote\", \"crowd_layer\", \"trace_reg\", \"conal\", \"union_net\", \"madl\", \"geo_reg_w\", \"geo_reg_f\", \"crowdar\", \"annot_mix\"]\n",
    "VARIANTS = [\"worst-1\", \"worst-2\", \"worst-var\", \"rand-1\", \"rand-2\", \"rand-var\", \"full\"]\n",
    "\n",
    "# # Increase maximum number of displayed rows.\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "def evaluate(mlruns_path: str, experiment_name: str, update_columns: dict = None, perf_type: str = \"gt\", version: str = \"test\", metric: str = \"acc\", epoch: str = \"best\"):\n",
    "    \"\"\"\n",
    "    Queries the evaluation results via mlflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mlruns_path : str  \n",
    "        Path to the results saved via mlflow.\n",
    "    experiment_name : str\n",
    "        Name of the mlflow experiment.\n",
    "    update_columns : dict, default=None\n",
    "        Optional dictionary of columns to be included in the outputted table of results.\n",
    "    perf_type : str, default=\"gt\"\n",
    "        Either 'gt' representing the classification models estimates of the ground truth (gt) class labels or 'ap' representing the annotator models' estimates of the \n",
    "        annotators' performances.\n",
    "    version : str, default=\"str\"\n",
    "        Either 'train', 'valid', or 'test' representing the results for different subsets.\n",
    "    metric : str, default=\"acc\"\n",
    "        Either 'acc', 'brier_score', or 'tce' as the two used performance metrics.\n",
    "    epoch : str\n",
    "        Show the results either after the 'last' or 'best' epoch.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    runs: pd.DataFrame\n",
    "        Table of results.\n",
    "    \"\"\"\n",
    "    set_tracking_uri(uri=f\"file://{to_absolute_path(mlruns_path)}\")\n",
    "    exp = get_experiment_by_name(experiment_name)\n",
    "    if exp is None:\n",
    "        return None\n",
    "    query = \"status = 'FINISHED'\"\n",
    "    runs = search_runs(experiment_ids=exp.experiment_id, filter_string=query, output_format=\"pandas\")\n",
    "    if len(runs) == 0:\n",
    "        return None\n",
    "    columns = {\n",
    "        \"params.data.class_definition._target_\": \"data\",\n",
    "        \"params.classifier.name\": \"clf\",\n",
    "        \"params.classifier.aggregation_method\": \"agg\",\n",
    "    }\n",
    "    if update_columns is not None:\n",
    "        columns.update(update_columns)\n",
    "    aggregation_dict = {f\"metrics.{perf_type}_{version}_{metric}_{epoch}\": [\"mean\", \"std\"], \"params.seed\": [\"sum\"]}\n",
    "    runs = runs.drop_duplicates(list(columns.keys()) + [\"params.seed\"]).fillna(\"--\")\n",
    "    runs = runs.sort_values(by=\"params.seed\")\n",
    "    runs = runs.groupby(list(columns.keys()), as_index=False).agg(aggregation_dict)\n",
    "    reindex_columns = [c1 + c2 for c1, c2 in runs.columns]\n",
    "    runs.columns = runs.columns.droplevel(level=1)\n",
    "    runs.columns = reindex_columns\n",
    "    for c in runs.columns:\n",
    "        if c.startswith(\"metrics\") and \"acc\" in c:\n",
    "            runs[c] = np.round(runs[c].values * 100 , 3)\n",
    "    columns[f\"params.seedsum\"] = \"n_runs\"\n",
    "    columns[f\"metrics.{perf_type}_{version}_{metric}_{epoch}mean\"] = f\"{version}-{perf_type}-{metric}-{epoch}-mean\"\n",
    "    columns[f\"metrics.{perf_type}_{version}_{metric}_{epoch}std\"] = f\"{version}-{perf_type}-{metric}-{epoch}-std\"\n",
    "    runs = runs.rename(columns=columns)\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6126324e5beefe8",
   "metadata": {},
   "source": [
    "## Hyperparameter Study\n",
    "\n",
    "Load the table with the results of the hyperparameter study with the ground truth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c61e2d50a6b082",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_columns = {\"params.data.optimizer.gt_params.lr\": \"lr\", \"params.data.train_batch_size\": \"bs\", \"params.data.optimizer.gt_params.weight_decay\": \"wd\"}\n",
    "runs_df = evaluate(mlruns_path=MLRUNS_PATH, experiment_name=\"hyperparameter_search\", update_columns=update_columns, perf_type=\"gt\", version=\"valid\", metric=\"acc\", epoch=\"best\")\n",
    "if runs_df is not None:\n",
    "    runs_df = runs_df.sort_values(by=['bs', 'wd', 'lr'])\n",
    "    runs_df[\"lr\"] = runs_df[\"lr\"].astype(float)\n",
    "    runs_df[\"bs\"] = runs_df[\"bs\"].astype(int)\n",
    "    runs_df[\"wd\"] = runs_df[\"wd\"].astype(float)\n",
    "    runs_df_compressed = runs_df.drop(columns=[\"data\", \"clf\", \"agg\"])\n",
    "    means_df = runs_df_compressed.pivot(index=\"lr\", columns=[\"bs\", \"wd\"], values=f\"valid-gt-acc-best-mean\")\n",
    "    std_df = runs_df_compressed.pivot(index=\"lr\", columns=[\"bs\", \"wd\"], values=f\"valid-gt-acc-best-std\")\n",
    "    merged_df = means_df.applymap(lambda x: '')  # Create an empty DataFrame with the same structure\n",
    "    if SHOW_LATEX:\n",
    "            for col in means_df.columns:\n",
    "                for idx in means_df.index:\n",
    "                        merged_df.at[idx, col] = f\"${means_df.at[idx, col]:.1f}_{{\\pm {std_df.at[idx, col]:.1f}}}$\"\n",
    "            latex = merged_df.apply(lambda row: ' & '.join(row.values.astype(str)), axis=1)\n",
    "            print(latex.to_markdown())\n",
    "    else:\n",
    "        for col in means_df.columns:\n",
    "            for idx in means_df.index:\n",
    "                    merged_df.at[idx, col] = f\"{means_df.at[idx, col]:.1f} +- {std_df.at[idx, col]:.1f}\"\n",
    "        print(merged_df.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808ff27-dc46-45a6-aa55-0b1ea99fbf5e",
   "metadata": {},
   "source": [
    "List the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62266387-5e6f-4dee-bcd0-061eead112f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if runs_df is not None:\n",
    "    val_acc = [runs_df[f\"valid-gt-acc-best-mean\"].values]\n",
    "    val_acc = np.mean(val_acc, axis=0)\n",
    "    best_idx = np.argmax(val_acc)\n",
    "    print(runs_df.iloc[best_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f7d40-18bb-49b2-9eec-6b056834da6b",
   "metadata": {},
   "source": [
    "## Benchmark and Case Studies\n",
    "\n",
    "Load the table with the results of the benchmark or case studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8aebe448695ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Define the experiment name. Possible experiments are: \"benchmark\", \"beyond_hard_labels\", or \"annotator_metadata\".\n",
    "experiment_name = \"benchmark\" \n",
    "\n",
    "# Load empirical results.\n",
    "ranks_dict = {}\n",
    "perf_type = \"gt\"\n",
    "version = \"test\"\n",
    "ranks_df = None\n",
    "for metric, ascending in zip([\"acc\", \"brier_score\", \"tce\"], [False, True, True]):\n",
    "    all_runs = []\n",
    "    full_metric_name = f'{version}-{perf_type}-{metric}-last'\n",
    "    for v in VARIANTS:\n",
    "        runs = evaluate(mlruns_path=MLRUNS_PATH, experiment_name=f\"{experiment_name}_{v}\", update_columns={\"params.data.class_definition.variant\": \"variant\"}, perf_type=perf_type, version=version, metric=metric, epoch=\"last\")\n",
    "        if runs is None:\n",
    "            continue\n",
    "        new_runs = runs.drop_duplicates(subset=[\"data\", \"variant\", \"clf\", \"agg\"], keep=\"first\")\n",
    "        all_runs.append(new_runs)\n",
    "    if len(all_runs):\n",
    "        runs_df = pd.concat(all_runs)\n",
    "        ranks_df = runs_df[runs_df[\"agg\"] != \"ground-truth\"].copy()\n",
    "        ranks_df[\"rank\"] = ranks_df.groupby('variant')[f\"{full_metric_name}-mean\"].rank(ascending=ascending, method=\"min\")\n",
    "        ranks_df = pd.DataFrame(ranks_df.groupby(\"clf\")[\"rank\"].mean())\n",
    "        ranks_df = ranks_df.reindex(APPROACHES[1:])\n",
    "        ranks_dict[metric] = ranks_df[\"rank\"].values\n",
    "        print(f\"\\n############################ {version.upper()} {metric.upper()} ############################\")\n",
    "        \n",
    "        means_df = runs_df.copy()\n",
    "        means_df[\"approach\"] = means_df[['clf', 'agg']].agg(' '.join, axis=1) \n",
    "        means_df[\"approach\"] = [approach.replace(\"aggregate \", \"\").replace(\" None\", \"\") for approach in means_df[\"approach\"].values]\n",
    "        means_df = means_df.pivot(index=\"variant\", columns=\"approach\", values=f\"{full_metric_name}-mean\")\n",
    "        means_df = means_df.reindex(columns=APPROACHES, index=VARIANTS)\n",
    "        def max_and_second_max(row):\n",
    "            sorted_cols = row.sort_values(ascending=False).index\n",
    "            return pd.Series([sorted_cols[0], sorted_cols[1]], index=['max_col', 'second_max_col'])\n",
    "        best_ids = means_df[APPROACHES[1:]].apply(lambda row: max_and_second_max(row), axis=1)\n",
    "        \n",
    "        std_df = runs_df.copy()\n",
    "        std_df[\"approach\"] = std_df[['clf', 'agg']].agg(' '.join, axis=1) \n",
    "        std_df[\"approach\"] = [approach.replace(\"aggregate \", \"\").replace(\" None\", \"\") for approach in std_df[\"approach\"].values]\n",
    "        std_df = std_df.pivot(index=\"variant\", columns=\"approach\", values=f\"{full_metric_name}-std\")\n",
    "        std_df = std_df.reindex(columns=APPROACHES, index=VARIANTS)\n",
    "            \n",
    "        if SHOW_LATEX:\n",
    "            merged_df = means_df.applymap(lambda x: '')  # Create an empty DataFrame with the same structure\n",
    "            for col in means_df.columns:\n",
    "                for idx in means_df.index:\n",
    "                    if col == \"ground-truth\":\n",
    "                        merged_df.at[idx, col] = f\"${{\\color{{partialcolor}}{means_df.at[idx, col]:.1f}_{{\\pm {std_df.at[idx, col]:.1f}}}}}$\"\n",
    "                    elif col == best_ids.loc[idx].values[0]:\n",
    "                        merged_df.at[idx, col] = f\"\\\\textBF{{${{\\color{{color_annot_green}} \\\\text{{{means_df.at[idx, col]:.1f}}}_{{\\pm {std_df.at[idx, col]:.1f}}}}}$}}\"\n",
    "                    elif col == best_ids.loc[idx].values[1]:\n",
    "                        merged_df.at[idx, col] = f\"\\\\textBF{{${{\\color{{color_annot_violet}} \\\\text{{{means_df.at[idx, col]:.1f}}}_{{\\pm {std_df.at[idx, col]:.1f}}}}}$}}\"\n",
    "                    else:\n",
    "                        merged_df.at[idx, col] = f\"${means_df.at[idx, col]:.1f}_{{\\pm {std_df.at[idx, col]:.1f}}}$\"\n",
    "            latex = merged_df.apply(lambda row: ' & '.join(row.values.astype(str)), axis=1)\n",
    "            print(latex.to_markdown())\n",
    "        else:\n",
    "            merged_df = means_df.applymap(lambda x: '')  # Create an empty DataFrame with the same structure\n",
    "            for col in means_df.columns:\n",
    "                for idx in means_df.index:\n",
    "                    if metric == \"acc\":\n",
    "                        merged_df.at[idx, col] = f\"{means_df.at[idx, col]:.1f} +- {std_df.at[idx, col]:.1f}\"\n",
    "                    else:\n",
    "                        merged_df.at[idx, col] = f\"{means_df.at[idx, col]:.2f} +- {std_df.at[idx, col]:.2f}\"\n",
    "            print(merged_df.to_markdown())\n",
    "    \n",
    "if experiment_name == \"benchmark\" and ranks_df is not None:\n",
    "    # Define the colors\n",
    "    colors = ['#007d7d99', '#7f007fff']\n",
    "\n",
    "    # Create a custom colormap\n",
    "    cmap = LinearSegmentedColormap.from_list('custom_diverging', colors, N=256)\n",
    "\n",
    "    ranks = pd.DataFrame(np.column_stack((ranks_dict[\"acc\"], ranks_dict[\"brier_score\"], ranks_dict[\"tce\"])), index=APPROACHES[1:], columns=[\"acc\", \"brier\", \"tce\"])\n",
    "    #ranks = ranks.reindex([\"aggregate\", \"crowd_layer\", \"trace_reg\", \"conal\", \"union_net\", \"madl\", \"geo_reg_w\", \"geo_reg_f\", \"crowdar\", \"annot_mix\"])\n",
    "    fig = plt.figure(figsize=(3, 7), dpi=100)\n",
    "    ax = fig.add_axes(\n",
    "        [0.1, 0.1, 0.8, 0.8]\n",
    "    )\n",
    "    sns.heatmap(ranks, annot=True, cmap=cmap, cbar=False, ax=ax, annot_kws={\"size\": 12}, fmt=\".3g\", alpha=.6)\n",
    "    plt.savefig(\"ranks.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
