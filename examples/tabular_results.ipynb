{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tabular Results\n",
    "\n",
    "This notebook allows us to query tabular results via [`mlflow`](https://mlflow.org/). As a prerequisite, the experiments have to be performed in a first step. If this is the case, we can load the results for the different tables in the accompanied article.\n",
    "\n",
    "Update the `mlruns_path` to the path used in your config file [`experiment.yaml`](../conf/experiment.yaml)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c074a60a078dedc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hydra.utils import to_absolute_path\n",
    "from mlflow import set_tracking_uri, get_experiment_by_name, search_runs\n",
    "\n",
    "def evaluate(mlruns_path=\"/mnt/work/anonymous/maml/mlruns\", experiment_name=\"cifar_10_h\", update_columns=None, perf_type=\"gt\", version=\"test\", metric=\"acc\", epoch=\"best\"):\n",
    "    \"\"\"\n",
    "    Queries the evaluation results via mlflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mlruns_path : str  \n",
    "        Path to the results saved via mlflow.\n",
    "    experiment_name : str\n",
    "        Name of the mlflow experiment.\n",
    "    update_columns : dict\n",
    "        Optional dictionary of columns to be included in the outputted table of results.\n",
    "    perf_type : str\n",
    "        Either 'gt' representing the classification models estimates of the ground truth (gt) class labels or 'ap' representing the annotator models' estimates the \n",
    "        annotators' performances.\n",
    "    version : str\n",
    "        Either 'train', 'valid', or 'test' representing the results for different subsets.\n",
    "    metric : str\n",
    "        Either 'acc' or 'auroc' as the two used performance metrics.\n",
    "    epoch : str\n",
    "        Show the results either after the 'last' or 'best' epoch.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    runs: pd.DataFrame\n",
    "        Table of results.\n",
    "    \"\"\"\n",
    "    set_tracking_uri(uri=f\"file://{to_absolute_path(mlruns_path)}\")\n",
    "    exp = get_experiment_by_name(experiment_name)\n",
    "    if exp is None:\n",
    "        return None\n",
    "    query = \"status = 'FINISHED' and params.classifier.name != 'aggregate'\"\n",
    "    runs = search_runs(experiment_ids=exp.experiment_id, filter_string=query, output_format=\"pandas\")\n",
    "    if len(runs) == 0:\n",
    "        return None\n",
    "    columns = {\n",
    "        \"params.data.class_definition._target_\": \"data\",\n",
    "        \"params.classifier.name\": \"clf\",\n",
    "        \"params.classifier.aggregation_method\": \"agg\",\n",
    "    }\n",
    "    if update_columns is not None:\n",
    "        columns.update(update_columns)\n",
    "    aggregation_dict = {f\"metrics.{perf_type}_{version}_{metric}_{epoch}\": [\"mean\", \"std\"], \"params.seed\": [\"sum\"]}\n",
    "    runs = runs.drop_duplicates(list(columns.keys()) + [\"params.seed\"]).fillna(\"--\")\n",
    "    runs = runs.groupby(list(columns.keys()), as_index=False).agg(aggregation_dict)\n",
    "    reindex_columns = [c1 + c2 for c1, c2 in runs.columns]\n",
    "    runs.columns = runs.columns.droplevel(level=1)\n",
    "    runs.columns = reindex_columns\n",
    "    for c in runs.columns:\n",
    "        if c.startswith(\"metrics\"):\n",
    "            runs[c] = np.round(runs[c].values * 100 , 3)\n",
    "    columns[f\"params.seedsum\"] = \"n_runs\"\n",
    "    columns[f\"metrics.{perf_type}_{version}_{metric}_{epoch}mean\"] = f\"{version}-{perf_type}-{metric}-{epoch}-mean\"\n",
    "    columns[f\"metrics.{perf_type}_{version}_{metric}_{epoch}std\"] = f\"{version}-{perf_type}-{metric}-{epoch}-std\"\n",
    "    runs = runs.rename(columns=columns)\n",
    "    return runs\n",
    "\n",
    "# Increase maximum number of displayed rows.\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# List of all dataset names.\n",
    "datasets = [\n",
    "    \"music_genres\",\n",
    "    \"label_me\",\n",
    "    \"cifar_10_h\",\n",
    "    \"cifar_10_n\",\n",
    "    \"cifar_100_n\",\n",
    "    \"letter_sim\",\n",
    "    \"flowers_102_sim\",\n",
    "    \"trec_6_sim\",\n",
    "    \"aloi_sim\",\n",
    "    \"dtd_sim\",\n",
    "    \"ag_news_sim\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmark Study\n",
    "\n",
    "Load the table with the results of the benchmark study for the classification models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6126324e5beefe8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "for e in [\"last\", \"best\"]:\n",
    "    print(f\"\\n\\n{e}\")\n",
    "    for d in datasets:\n",
    "        runs = evaluate(experiment_name=d+\"_benchmark\", perf_type=\"gt\", version=\"test\", metric=\"acc\", epoch=e)\n",
    "        if runs is None:\n",
    "            continue\n",
    "        new_runs = runs.drop_duplicates(subset=[\"data\", \"clf\", \"agg\"], keep=\"first\")\n",
    "        all_runs.append(new_runs)\n",
    "    if len(all_runs):\n",
    "        runs_df = pd.concat(all_runs)\n",
    "    print(runs_df.to_markdown())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41c61e2d50a6b082"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the table with the results of the benchmark study for the annotator models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f089e03ca6216a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "for e in [\"last\", \"best\"]:\n",
    "    print(f\"\\n\\n{e}\")\n",
    "    for d in datasets:\n",
    "        runs = evaluate(experiment_name=d+\"_benchmark\", perf_type=\"ap\", version=\"test\", metric=\"auroc\", epoch=e)\n",
    "        if runs is None:\n",
    "            continue\n",
    "        new_runs = runs.drop_duplicates(subset=[\"data\", \"clf\", \"agg\"], keep=\"first\")\n",
    "        all_runs.append(new_runs)\n",
    "    if len(all_runs):\n",
    "        runs_df = pd.concat(all_runs)\n",
    "    print(runs_df.to_markdown())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15b8aebe448695ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ablation Study\n",
    "\n",
    "Load the table with the results of the ablation study for the classification model of annot-mix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f700ea41d18200"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "alpha_column = {\"params.classifier.params.alpha\": \"alpha\"}\n",
    "for e in [\"last\", \"best\"]:\n",
    "    print(f\"\\n\\n{e}\")\n",
    "    for d in datasets:\n",
    "        runs = evaluate(experiment_name=d+\"_ablation\", perf_type=\"gt\", version=\"test\", metric=\"acc\", update_columns=alpha_column, epoch=e)\n",
    "        if runs is None:\n",
    "            continue\n",
    "        new_runs = runs.drop_duplicates(subset=[\"data\", \"clf\", \"agg\"] + list(alpha_column.values()), keep=\"first\")\n",
    "        all_runs.append(new_runs)\n",
    "    if len(all_runs):\n",
    "        runs_df = pd.concat(all_runs)\n",
    "    print(runs_df.to_markdown())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76d764772b51166b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
